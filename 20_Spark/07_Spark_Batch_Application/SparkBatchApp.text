# Creating a Spark Batch Application


// build.sbt file
name := "SparkApp1"
version := "0.1"
scalaVersion := "2.12.12"

libraryDependencies ++= Seq(
    "org.apache.spark" %% "spark-sql" % "3.1.1"
)


// WordCount.scala
import org.apache.spark.sql.SparkSession
import org.apache.spark.rdd.RDD

object WordCount {
    def main(args: Array[String]) {
        val file = "Gettysburg-Address.txt"
        val spark: SparkSession = SparkSession.builder
                                              .appName("Word Count")
                                              .config("spark.master", "local")
                                              .getOrCreate()
        val fileRdd: RDD[String] = spark.sparkContext.textFile(file)
        val counts = fileRdd.map(_.replaceAll("[.,]", ""))
                            .map(_.replace("â€”", " "))
                            .flatMap(line => line.split(" "))
                            .map(word => (word, 1))
                            .reduceByKey(_ + _)
                            .sortBy(_._2)
                            .collect

        println( "------------------------------------------")
        counts.foreach(println)
        println( "------------------------------------------")

        spark.stop()
    }

}


// build the JAR file for your application
sbt package

// run it with the `spark-submit` command
$ spark-submit --class WordCount target/scala-2.12/sparkapp1_2.12-0.1.jar


